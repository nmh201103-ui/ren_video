import re
import time
from typing import Dict, List
from io import BytesIO
import requests
from PIL import Image
import numpy as np
from playwright.sync_api import sync_playwright
from scraper.base import BaseScraper
from utils.logger import get_logger

logger = get_logger()

class ShopeeScraper(BaseScraper):
    MAX_DESC_RETRIES = 8  # TƒÉng s·ªë l·∫ßn th·ª≠ cu·ªôn
    SCROLL_STEP = 1000    # Cu·ªôn m·∫°nh h∆°n ƒë·ªÉ k√≠ch ho·∫°t lazy load

    def scrape(self, url: str) -> Dict:
        logger.info(f"üîó K·∫øt n·ªëi t·ªõi Chrome (9222) - URL: {url}")
        with sync_playwright() as p:
            try:
                # K·∫øt n·ªëi browser ƒëang m·ªü
                browser = p.chromium.connect_over_cdp("http://localhost:9222")
                context = browser.contexts[0]
                
                page = next((pg for pg in context.pages if "shopee.vn" in pg.url), None)
                if not page:
                    page = context.new_page()

                ua = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
                page.set_extra_http_headers({"User-Agent": ua})

                if url not in page.url:
                    page.goto(url, wait_until="domcontentloaded", timeout=60000)
                
                # ƒê·ª£i trang ·ªïn ƒë·ªãnh m·ªôt ch√∫t
                page.wait_for_timeout(3000)

                # Ki·ªÉm tra Shopee redirect sang captcha/x√°c minh ‚Äî kh√¥ng scrape ƒë∆∞·ª£c
                current_url = page.url or ""
                if "/verify/captcha" in current_url or "/verify" in current_url:
                    logger.error("‚ùå Shopee ƒëang hi·ªÉn th·ªã trang captcha/x√°c minh. Ho√†n th√†nh captcha trong Chrome r·ªìi th·ª≠ l·∫°i.")
                    out = self._empty_data(url)
                    out["_scrape_failed"] = True
                    out["_scrape_error"] = (
                        "Shopee chuy·ªÉn sang trang captcha/x√°c minh. "
                        "B·∫°n c·∫ßn: 1) M·ªü link s·∫£n ph·∫©m trong Chrome, 2) B·∫•m 'Th·ª≠ L·∫°i' / ho√†n th√†nh captcha (n·∫øu c√≥), "
                        "3) ƒê·ª£i trang s·∫£n ph·∫©m load xong, 4) B·∫•m Generate l·∫°i."
                    )
                    return out

                # 1. L·∫•y Title, ·∫¢nh v√† Gi√°
                title = self._get_title(page)
                images = self._get_images_advanced(page)
                price = self._get_price(page)

                # 2. CHI·∫æN THU·∫¨T QU√âT M√î T·∫¢ TRI·ªÜT ƒê·ªÇ
                description = ""
                
                # C·ªë g·∫Øng t√¨m v√† b·∫•m n√∫t "Xem th√™m" n·∫øu c√≥ ƒë·ªÉ Shopee bung full text
                try:
                    expand_button = page.locator('button:has-text("Xem th√™m"), div:has-text("Xem th√™m")').last
                    if expand_button.is_visible():
                        expand_button.click(timeout=3000)
                        page.wait_for_timeout(1000)
                except:
                    pass

                for attempt in range(self.MAX_DESC_RETRIES):
                    # Cu·ªôn chu·ªôt s√¢u xu·ªëng d∆∞·ªõi
                    page.mouse.wheel(0, self.SCROLL_STEP)
                    page.wait_for_timeout(1000)
                    
                    description = self._get_description_logic(page)
                    # N·∫øu l·∫•y ƒë∆∞·ª£c tr√™n 500 k√Ω t·ª± (m√¥ t·∫£ th·∫≠t th∆∞·ªùng d√†i) th√¨ d·ª´ng
                    if len(description) > 500:
                        break
                
                # Fallback n·∫øu v·∫´n r·ªóng ho·∫∑c qu√° ng·∫Øn
                if len(description) < 100:
                    logger.warning("‚ö†Ô∏è Selector chuy√™n s√¢u th·∫•t b·∫°i, l·∫•y d·ªØ li·ªáu th√¥ t·ª´ Meta ho·∫∑c Body...")
                    description = self._get_fallback_description(page)

                # 3. L√†m s·∫°ch nh·∫π nh√†ng (gi·ªØ nguy√™n c·∫•u tr√∫c xu·ªëng d√≤ng ƒë·ªÉ Renderer t√°ch c√¢u)
                clean_desc = description.replace("M√î T·∫¢ S·∫¢N PH·∫®M", "").strip()

                # Fallback gi√° t·ª´ m√¥ t·∫£ n·∫øu DOM kh√¥ng l·∫•y ƒë∆∞·ª£c
                if (not price or price == "0") and clean_desc:
                    price_match = re.search(r"(?:gi√°|Gi√°|GI√Å|‚Ç´|VNƒê|vnd)\s*[:\s]*([\d.,]+)\s*(?:‚Ç´|VNƒê|k)?", clean_desc, re.IGNORECASE)
                    if not price_match:
                        price_match = re.search(r"\b(\d{2,3}(?:\.\d{3})+(?:\.\d{3})?)\s*‚Ç´", clean_desc)
                    if price_match:
                        pstr = re.sub(r"[^\d]", "", price_match.group(1))
                        if len(pstr) >= 4:
                            price = pstr
                            logger.info("üìå L·∫•y gi√° t·ª´ m√¥ t·∫£ (fallback): %s", price)

                # N·∫øu 0 ·∫£nh + title generic ‚Üí c√≥ th·ªÉ ƒëang trang captcha/l·ªói t·∫£i
                generic_titles = ("S·∫£n ph·∫©m Shopee", "Shopee", "Shopee Vi·ªát Nam")
                if len(images) == 0 and (not title or title.strip() in generic_titles):
                    logger.warning("‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c ·∫£nh v√† title ‚Äî c√≥ th·ªÉ trang captcha ho·∫∑c 'L·ªói t·∫£i'. Ho√†n th√†nh x√°c minh trong Chrome r·ªìi th·ª≠ l·∫°i.")
                    out = self._empty_data(url)
                    out["_scrape_failed"] = True
                    out["_scrape_error"] = (
                        "Kh√¥ng l·∫•y ƒë∆∞·ª£c ·∫£nh s·∫£n ph·∫©m (trang c√≥ th·ªÉ ƒëang captcha ho·∫∑c 'L·ªói t·∫£i'). "
                        "Trong Chrome: b·∫•m 'Th·ª≠ L·∫°i' / ho√†n th√†nh captcha, ƒë·ª£i trang s·∫£n ph·∫©m load xong r·ªìi Generate l·∫°i."
                    )
                    return out

                # --- LOG KI·ªÇM TRA ---
                logger.info('[SCRAPER COMPLETED]')
                logger.info('Title: %s', title[:60])
                logger.info('Images: %d', len(images))
                logger.info('Price: %s', price)
                logger.info('Description length: %d chars', len(clean_desc))

                return {
                    "title": title,
                    "image_urls": images,
                    "description": clean_desc,
                    "short_description": clean_desc,
                    "price": price,
                    "platform": "shopee",
                    "original_url": url
                }

            except Exception as e:
                logger.error(f"‚ùå Scraper Error: {e}")
                out = self._empty_data(url)
                out["_scrape_failed"] = True
                out["_scrape_error"] = str(e)
                return out

    def _get_description_logic(self, page) -> str:
        try:
            # Cu·ªôn ƒë·∫øn gi·ªØa trang ƒë·ªÉ k√≠ch ho·∫°t Lazy Load c·ªßa ph·∫ßn m√¥ t·∫£
            page.evaluate("window.scrollTo(0, document.body.scrollHeight / 3)")
            page.wait_for_timeout(2000)

            return page.evaluate("""() => {
                // 1. T√¨m ch√≠nh x√°c Container ch·ª©a chi ti·∫øt s·∫£n ph·∫©m (C·∫≠p nh·∫≠t 2024-2025)
                // Shopee th∆∞·ªùng ƒë·∫∑t m√¥ t·∫£ trong c√°c class c√≥ c·∫•u tr√∫c renderer ho·∫∑c p_7rWz
                const specificSelectors = [
                    'div.product-detail__renderer', 
                    'div.p_7rWz',
                    '.product-details__section',
                    'div._3yZ_0n'
                ];

                for (let sel of specificSelectors) {
                    const el = document.querySelector(sel);
                    if (el) {
                        const txt = el.innerText || '';
                        const up = txt.toUpperCase();
                        // T√¨m c√°c bi·∫øn th·ªÉ: "M√î T·∫¢", "M√î T·∫¢ S·∫¢N PH·∫®M" ho·∫∑c "PRODUCT DESCRIPTION"
                        if (up.includes("M√î T·∫¢") || up.includes("PRODUCT DESCRIPTION")) {
                            // Lo·∫°i b·ªè ph·∫ßn ƒë√°nh gi√° n·∫øu c√≥
                            let content = txt.split(/ƒê√ÅNH GI√Å S·∫¢N PH·∫®M/i)[0];
                            return content.trim();
                        }
                    }
                }

                // 2. N·∫øu kh√¥ng t√¨m th·∫•y b·∫±ng class, t√¨m d·ª±a tr√™n ti√™u ƒë·ªÅ vƒÉn b·∫£n (case-insensitive)
                const allElements = document.querySelectorAll('h2, div, section, span');
                for (let el of allElements) {
                    const txt = (el.innerText || '').trim();
                    const up = txt.toUpperCase();
                    if (up === "M√î T·∫¢ S·∫¢N PH·∫®M" || up === "PRODUCT DESCRIPTION" || up.includes('M√î T·∫¢')) {
                        const nextEl = el.nextElementSibling || el.parentElement;
                        if (nextEl) {
                            let content = nextEl.innerText || '';
                            content = content.split(/ƒê√ÅNH GI√Å S·∫¢N PH·∫®M/i)[0].trim();
                            if (content) return content;
                        }
                    }
                }
                return "";
            }""")
        except:
            return ""

    def _get_fallback_description(self, page) -> str:
        """H√†m d·ª± ph√≤ng nh∆∞ng c√≥ gi·ªõi h·∫°n ph·∫°m vi ƒë·ªÉ tr√°nh l·∫•y 6000+ k√Ω t·ª± r√°c
        T√°ch ph·∫ßn logic ph·ª©c t·∫°p ƒë·ªÉ d·ªÖ unit-test b·∫±ng Python.
        """
        try:
            # L·∫•y to√†n b·ªô text c·ªßa body t·ª´ trang
            body_text = page.evaluate("() => document.body.innerText") or ""
            desc = self._extract_description_from_body_text(body_text)
            if desc:
                return desc
            # N·∫øu kh√¥ng c√≥, th·ª≠ meta description
            meta = page.evaluate("() => document.querySelector('meta[name=\"description\"]')?.content")
            return meta or ""
        except Exception:
            return ""

    def _extract_description_from_body_text(self, body_text: str) -> str:
        """T√°ch v√† tr·∫£ v·ªÅ ƒëo·∫°n m√¥ t·∫£ t·ª´ body text (case-insensitive)."""
        if not body_text:
            return ""
        body_up = body_text.upper()
        start_idx = body_up.find('M√î T·∫¢')
        if start_idx == -1:
            start_idx = body_up.find('PRODUCT DESCRIPTION')
        if start_idx != -1:
            end_idx = body_up.find('ƒê√ÅNH GI√Å S·∫¢N PH·∫®M', start_idx)
            if end_idx == -1 or (end_idx - start_idx) > 2500:
                end_idx = start_idx + 2000
            result = body_text[start_idx:end_idx]
            # Lo·∫°i b·ªè c√°c ti√™u ƒë·ªÅ nh∆∞ 'M√î T·∫¢ S·∫¢N PH·∫®M', 'M√î T·∫¢' ho·∫∑c 'PRODUCT DESCRIPTION'
            result = re.sub(r"M√î T·∫¢ S·∫¢N PH·∫®M", "", result, flags=re.IGNORECASE)
            result = re.sub(r"M√î T·∫¢", "", result, flags=re.IGNORECASE)
            result = re.sub(r"PRODUCT DESCRIPTION", "", result, flags=re.IGNORECASE)
            return result.strip()
        return ""

    def _get_title(self, page) -> str:
        try:
            # ∆Øu ti√™n og:title (trang s·∫£n ph·∫©m lu√¥n c√≥), tr√°nh title tab generic "Shopee Vi·ªát Nam"
            og = page.evaluate("document.querySelector('meta[property=\"og:title\"]')?.content") or ""
            if og and len(og.strip()) > 10 and "shopee" not in og.strip().lower()[:20]:
                return og.strip().split("|")[0].strip()
            t = page.title().split("|")[0].strip()
            if not t or t == "Shopee" or "Shopee Vi·ªát Nam" in t or len(t) < 8:
                return og.strip().split("|")[0].strip() if og else "S·∫£n ph·∫©m Shopee"
            return t
        except Exception:
            return "S·∫£n ph·∫©m Shopee"

    def _get_images_advanced(self, page) -> List[str]:
        try:
            # Cu·ªôn t·ªõi v√πng gallery ƒë·ªÉ k√≠ch lazy-load ·∫£nh s·∫£n ph·∫©m
            try:
                page.evaluate("window.scrollTo(0, 400)")
                page.wait_for_timeout(1500)
            except Exception:
                pass
            raw_urls = page.evaluate("""() => {
                const urls = new Set();
                const add = (src) => {
                    if (!src || typeof src !== 'string') return;
                    const s = src.trim();
                    // Shopee CDN: usercontent, shopee.com/file, cdn.shopee, down-*.img
                    if (s.includes('usercontent') || s.includes('shopee.com/file') || s.includes('cdn.shopee') || /down-[a-z0-9-]+\\.img\\./.test(s) || (s.includes('shopee') && s.includes('img'))) {
                        let clean = s.split('@')[0].split('_tn')[0].split('_v')[0];
                        if (!clean.startsWith('http')) clean = 'https:' + clean;
                        urls.add(clean);
                    }
                };
                document.querySelectorAll('img').forEach(img => {
                    add(img.getAttribute('data-src') || img.getAttribute('src'));
                });
                document.querySelectorAll('picture source').forEach(el => { add(el.getAttribute('srcset')?.split(' ')[0]); add(el.getAttribute('src')); });
                return Array.from(urls).slice(0, 15);
            }""")
            # Fallback: ·∫£nh ch√≠nh t·ª´ og:image
            if not raw_urls or len(raw_urls) == 0:
                try:
                    og_img = page.evaluate("document.querySelector('meta[property=\"og:image\"]')?.content") or ""
                    if og_img and "shopee" in og_img.lower():
                        raw_urls = [og_img]
                        logger.info("üì∑ D√πng og:image l√†m ·∫£nh s·∫£n ph·∫©m (fallback)")
                except Exception:
                    pass
            
            # L·ªçc ·∫£nh s·∫£n ph·∫©m th·ª±c (lo·∫°i b·ªè banner, voucher, model)
            filtered = self._filter_product_images(raw_urls)
            logger.info(f"üì∏ L·ªçc ·∫£nh: {len(raw_urls)} -> {len(filtered)} ·∫£nh s·∫£n ph·∫©m")
            # B·ªè ·∫£nh th·ª© 3 (index 2): trong DOM th∆∞·ªùng l√† video/banner "GI·∫¢M" ‚Üí render ra tr·∫Øng khi l√†m video
            if len(filtered) > 2:
                filtered = filtered[:2] + filtered[3:]
                logger.info("üì∑ ƒê√£ b·ªè ·∫£nh th·ª© 3 (video/banner) ƒë·ªÉ tr√°nh render tr·∫Øng")
            return filtered[:10]  # Tr·∫£ v·ªÅ max 10 ·∫£nh
        except Exception as e:
            logger.error(f"‚ùå Failed to get images: {e}")
            return []
    
    def _filter_product_images(self, image_urls: List[str]) -> List[str]:
        """Filter to keep only real product images, remove banners/vouchers/models"""
        filtered = []
        
        for url in image_urls:
            try:
                if self._is_product_image(url):
                    filtered.append(url)
            except Exception as e:
                logger.debug(f"‚ö†Ô∏è Failed to check image {url}: {e}")
                # If check fails, keep it (safe default)
                filtered.append(url)
        
        return filtered
    
    def _is_product_image(self, url: str) -> bool:
        """
        Detect if image is real product photo (not banner/voucher/model).
        Returns True if likely a product image.
        """
        try:
            # CRITICAL: Only accept Shopee CDN URLs (block local files)
            if not url.startswith('http'):
                logger.debug(f"‚ùå Rejected: Not HTTP URL")
                return False
            
            # Block local file paths that might leak into scraper
            if 'file:///' in url or 'C:/' in url or 'C:' in url or 'Users/' in url:
                logger.debug(f"‚ùå Rejected: Local file path")
                return False
            
            # Must be from Shopee CDN (bao g·ªìm og:image, cdn, down-*.img)
            if not ('usercontent' in url or 'shopee.com/file/' in url or 'shopee' in url.lower() or 'cdn.shopee' in url or re.search(r'down-[a-z0-9-]+\.img\.', url)):
                logger.debug(f"‚ùå Rejected: Not Shopee CDN")
                return False
            
            # Download image
            response = requests.get(url, timeout=5, stream=True)
            response.raise_for_status()
            img = Image.open(BytesIO(response.content)).convert('RGB')
            
            # 1. Check aspect ratio (banners usually wide, products square-ish)
            width, height = img.size
            aspect_ratio = width / height
            
            # CH·ªà lo·∫°i b·ªè banner C·ª∞C K·ª≤ r√µ r√†ng (qu√° ngang)
            if aspect_ratio > 8.0 or aspect_ratio < 0.12:
                logger.debug(f"‚ùå Rejected: Extreme aspect ratio {aspect_ratio:.2f}")
                return False
            
            # 2. Check size - ch·ªâ lo·∫°i ·∫£nh icon c·ª±c nh·ªè
            if width < 100 or height < 100:
                logger.debug(f"‚ùå Rejected: Too small {width}x{height}")
                return False
            
            # T·∫ÆT text overlay check - qu√° nhi·ªÅu false positive
            # T·∫ÆT color distribution check - qu√° nhi·ªÅu false positive
            
            return True  # Pass most images
            
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Image check failed: {e}, keeping image")
            return True  # Safe default: keep if unable to check
    
    def _has_heavy_text_overlay(self, img: Image.Image) -> bool:
        """Detect if image has heavy text overlay (banner characteristic)"""
        try:
            # Convert to grayscale
            gray = img.convert('L')
            arr = np.array(gray)
            
            # Detect edges (text has many edges)
            from scipy import ndimage
            edges = ndimage.sobel(arr)
            edge_ratio = np.sum(edges > 40) / arr.size
            
            # If more than 30% of pixels are edges, likely has text
            return edge_ratio > 0.30
            
        except:
            return False  # If detection fails, assume no text
    
    def _is_single_color_dominant(self, img: Image.Image) -> bool:
        """Detect if single color dominates (voucher/icon characteristic)"""
        try:
            # Resize to speed up
            img_small = img.resize((100, 100))
            arr = np.array(img_small)
            
            # Check if one color channel dominates
            r_mean, g_mean, b_mean = arr[:,:,0].mean(), arr[:,:,1].mean(), arr[:,:,2].mean()
            total = r_mean + g_mean + b_mean
            
            # If one channel is > 70% of total, single color dominates
            if max(r_mean, g_mean, b_mean) / total > 0.7:
                return True
            
            # Check color variance
            variance = np.var(arr)
            if variance < 300:  # Low variance = single color
                return True
            
            return False
            
        except:
            return False

    def _empty_data(self, url: str) -> Dict:
        return {
            "title": "S·∫£n ph·∫©m Shopee",
            "image_urls": [],
            "description": "",
            "short_description": "",
            "price": "0",
            "platform": "shopee",
            "original_url": url,
            "_scrape_failed": False,
        }

    def _get_price(self, page) -> str:
        """L·∫•y gi√° s·∫£n ph·∫©m t·ª´ Shopee (nhi·ªÅu selector + JSON fallback)"""
        try:
            price = page.evaluate("""() => {
                function parsePriceNum(text) {
                    if (!text) return null;
                    const m = text.replace(/[.,\\s‚Ç´]/g, '').match(/\\d+/);
                    return m ? m[0] : null;
                }
                // 1. Selector gi√° hi·ªán t·∫°i (Shopee 2024-2025)
                const priceSelectors = [
                    '.product-price__current-price',
                    'span.shopee-price__current',
                    '[data-testid="product-price"]',
                    'div._3I7_6e',
                    '.pqTWkA', '.ZEgDHl', '.GcqTgR',
                    '[class*="price"] span',
                    '.shopee-product-info__main-price',
                    '.product-price'
                ];
                for (const sel of priceSelectors) {
                    const el = document.querySelector(sel);
                    if (el) {
                        const text = (el.innerText || el.textContent || '').trim();
                        const num = parsePriceNum(text);
                        if (num && num.length >= 4 && num.length <= 9) return num;
                    }
                }
                // 2. T·∫•t c·∫£ node ch·ª©a ‚Ç´
                const walker = document.createTreeWalker(document.body, NodeFilter.SHOW_TEXT);
                let node;
                while (node = walker.nextNode()) {
                    const t = node.textContent || '';
                    if (t.includes('‚Ç´')) {
                        const num = parsePriceNum(t);
                        if (num && num.length >= 4 && num.length <= 9) return num;
                    }
                }
                // 3. Regex to√†n trang (gi√° VND: 4-9 ch·ªØ s·ªë)
                const bodyText = document.body.innerText || '';
                const matches = bodyText.match(/[\\d.,]{4,}/g);
                if (matches) {
                    for (const m of matches) {
                        const num = m.replace(/[.,]/g, '');
                        if (num.length >= 4 && num.length <= 9 && parseInt(num, 10) >= 1000)
                            return num;
                    }
                }
                return '0';
            }""")
            return str(price).strip() if price else '0'
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Failed to get price: {e}")
            return '0'