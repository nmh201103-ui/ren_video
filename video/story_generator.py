"""Story/Narrative Script Generator - For storytelling videos"""
import os
import json
import subprocess
from utils.logger import get_logger

logger = get_logger()


class StoryScriptGenerator:
    """Generate storytelling narrative scripts from article content"""
    
    def __init__(self, use_llm=None):
        """
        Args:
            use_llm: "openai", "ollama", or None for heuristic
        """
        # Auto-detect LLM based on environment
        if use_llm is None:
            if os.getenv("OPENAI_API_KEY"):
                self.use_llm = "openai"
            elif self._has_ollama():
                self.use_llm = "ollama"
            else:
                self.use_llm = None
        else:
            self.use_llm = use_llm
        
        if self.use_llm == "openai":
            try:
                from openai import OpenAI
                self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
                logger.info("ü§ñ Story generator using OpenAI")
            except ImportError:
                self.use_llm = None
                logger.warning("OpenAI not available, using heuristic")
        elif self.use_llm == "ollama":
            logger.info("ü§ñ Story generator using Ollama")
        else:
            logger.info("üìù Story generator using heuristic")
    
    def _has_ollama(self):
        """Check if Ollama is installed"""
        try:
            subprocess.run(["ollama", "--version"], capture_output=True, check=True, timeout=5)
            return True
        except:
            return False
    
    def generate(self, title: str, description: str, content: str, max_scenes: int = None) -> list:
        """
        Generate storytelling script from article content
        Returns pure narrative without product mentions or CTAs
        
        Args:
            title: Article title
            description: Article description/excerpt
            content: Full article text
            max_scenes: Maximum number of scenes (auto-calculated if None)
        
        Returns:
            list of scene texts (each ~15-30s when spoken, PURE NARRATIVE)
        """
        
        logger.info(f"üìñ Generating story script: {title}")
        
        # Auto-calculate scenes based on content length if not specified
        if max_scenes is None:
            word_count = len(content.split())
            # ~20-30 seconds per scene, ~150 words per scene
            max_scenes = max(8, min(20, word_count // 150))
            logger.info(f"üìä Auto-calculated {max_scenes} scenes from {word_count} words")
        
        # Use LLM if available for better narrative
        if self.use_llm == "openai":
            try:
                script = self._generate_with_openai(title, description, content, max_scenes)
                if script:
                    logger.info(f"‚úÖ Generated {len(script)} AI-powered scenes (OpenAI)")
                    return script
            except Exception as e:
                logger.warning(f"OpenAI failed: {e}, falling back to heuristic")
        elif self.use_llm == "ollama":
            try:
                script = self._generate_with_ollama(title, description, content, max_scenes)
                if script:
                    logger.info(f"‚úÖ Generated {len(script)} AI-powered scenes (Ollama)")
                    return script
            except Exception as e:
                logger.warning(f"Ollama failed: {e}, falling back to heuristic")
        
        # Fallback: Heuristic approach
        return self._generate_heuristic(title, description, content, max_scenes)
    
    def _generate_with_openai(self, title: str, description: str, content: str, max_scenes: int) -> list:
        """Use OpenAI to create engaging narrative"""
        # Summarize for OpenAI too (though it has larger context)
        summarized_content = self._summarize_content(content, max_words=1200)
        
        prompt = f"""T·∫°o k·ªãch b·∫£n video k·ªÉ chuy·ªán t·ª´ b√†i vi·∫øt sau (THU·∫¶N T√öY K·ªÇ CHUY·ªÜN, KH√îNG QU·∫¢NG C√ÅO):

Ti√™u ƒë·ªÅ: {title}
M√¥ t·∫£: {description}
N·ªôi dung ch√≠nh: {summarized_content}

Y√™u c·∫ßu:
1. T·∫°o {max_scenes} ƒëo·∫°n k·ªãch b·∫£n (m·ªói ƒëo·∫°n ~20-30 gi√¢y khi ƒë·ªçc):
   - ƒêo·∫°n 1: Hook/M·ªü ƒë·∫ßu thu h√∫t
   - ƒêo·∫°n 2-{max_scenes-2}: N·ªôi dung ch√≠nh (k·ªÉ chuy·ªán t·ª± nhi√™n)
   - ƒêo·∫°n {max_scenes-1}: T√≥m t·∫Øt ƒëi·ªÉm ch√≠nh + l·ªùi khuy√™n √°p d·ª•ng
   - ƒêo·∫°n {max_scenes}: K·∫øt lu·∫≠n truy·ªÅn c·∫£m h·ª©ng (c·∫£m ∆°n + l·ªùi khuy√™n s√¢u s·∫Øc)

2. Gi·ªçng ƒëi·ªáu: T·ª± nhi√™n, g·∫ßn g≈©i, nh∆∞ ng∆∞·ªùi k·ªÉ chuy·ªán cho b·∫°n nghe
3. KH√îNG qu·∫£ng c√°o s·∫£n ph·∫©m, KH√îNG call-to-action
4. T·∫≠p trung v√†o n·ªôi dung c√¢u chuy·ªán/b√†i h·ªçc + c√°ch √°p d·ª•ng v√†o cu·ªôc s·ªëng

Tr·∫£ v·ªÅ JSON array g·ªìm {max_scenes} ƒëo·∫°n text ti·∫øng Vi·ªát. Ch·ªâ tr·∫£ JSON, kh√¥ng gi·∫£i th√≠ch."""

        response = self.client.chat.completions.create(
            model=os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"),
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )
        
        content_text = response.choices[0].message.content
        content_text = content_text.replace("```json", "").replace("```", "").strip()
        return json.loads(content_text)
    
    def _generate_with_ollama(self, title: str, description: str, content: str, max_scenes: int) -> list:
        """Use Ollama to create engaging narrative"""
        # Smart content summarization to fit Ollama context
        summarized_content = self._summarize_content(content, max_words=800)
        
        prompt = f"""T·∫°o k·ªãch b·∫£n video storytelling t·ª´ b√†i vi·∫øt (KH√îNG QU·∫¢NG C√ÅO):

Ti√™u ƒë·ªÅ: {title}
N·ªôi dung ch√≠nh: {summarized_content}

T·∫°o {max_scenes} ƒëo·∫°n k·ªãch b·∫£n (m·ªói ƒëo·∫°n ~20 gi√¢y):
1. Hook/M·ªü ƒë·∫ßu thu h√∫t
2-{max_scenes-2}. K·ªÉ chuy·ªán n·ªôi dung (t·ª± nhi√™n, g·∫ßn g≈©i)
{max_scenes-1}. T√≥m t·∫Øt + l·ªùi khuy√™n √°p d·ª•ng
{max_scenes}. K·∫øt lu·∫≠n truy·ªÅn c·∫£m h·ª©ng (c·∫£m ∆°n + insight s√¢u s·∫Øc)

Ch·ªâ k·ªÉ chuy·ªán/chia s·∫ª ki·∫øn th·ª©c, KH√îNG qu·∫£ng c√°o, KH√îNG b√°n h√†ng.
Gi·ªçng ƒëi·ªáu t·ª± nhi√™n, nh∆∞ ng∆∞·ªùi k·ªÉ chuy·ªán.

Tr·∫£ v·ªÅ JSON array [{max_scenes} ƒëo·∫°n text ti·∫øng Vi·ªát]. CH·ªà JSON, kh√¥ng th√™m text."""

        try:
            model = os.getenv("OLLAMA_MODEL", "gemma2:2b")
            timeout = int(os.getenv("OLLAMA_TIMEOUT", "120"))  # Increase timeout for longer content
            
            logger.info(f"ü§ñ Ollama: Using model {model}, timeout {timeout}s")
            
            result = subprocess.run(
                ["ollama", "run", model, prompt],
                capture_output=True,
                text=True,
                encoding="utf-8",
                timeout=timeout
            )
            
            output = result.stdout.strip()
            logger.info(f"üìù Ollama raw output length: {len(output)} chars")
            
            # Clean up markdown and extra text
            output = output.replace("```json", "").replace("```", "").strip()
            
            # Find JSON array in output (sometimes Ollama adds explanation)
            import re
            json_match = re.search(r'\[.*\]', output, re.DOTALL)
            if json_match:
                output = json_match.group(0)
            
            parsed = json.loads(output)
            
            # Validate it's a list
            if not isinstance(parsed, list):
                raise ValueError("Output is not a list")
            
            return parsed
            
        except subprocess.TimeoutExpired:
            logger.error(f"‚è±Ô∏è Ollama timeout after {timeout}s - content too long or model busy")
            return None
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Ollama JSON parse error: {e}")
            logger.debug(f"Raw output: {output[:500]}")
            return None
        except Exception as e:
            logger.error(f"‚ùå Ollama error: {e}")
            return None
    
    def _summarize_content(self, content: str, max_words: int = 800) -> str:
        """
        Smart content summarization to fit LLM context limits
        Instead of truncating, extract key paragraphs
        """
        # Split into paragraphs
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        
        # If content is short enough, return as-is
        current_words = len(content.split())
        if current_words <= max_words:
            return content
        
        # Extract key paragraphs (first, middle, last)
        if len(paragraphs) <= 3:
            summary = '\n\n'.join(paragraphs)
        else:
            # Take intro, some middle parts, and conclusion
            key_paragraphs = []
            key_paragraphs.append(paragraphs[0])  # Intro
            
            # Sample middle paragraphs
            middle_count = min(len(paragraphs) - 2, 3)
            step = (len(paragraphs) - 2) // middle_count if middle_count > 0 else 1
            for i in range(1, len(paragraphs) - 1, step):
                key_paragraphs.append(paragraphs[i])
                if len(key_paragraphs) >= 5:  # Limit total paragraphs
                    break
            
            key_paragraphs.append(paragraphs[-1])  # Conclusion
            summary = '\n\n'.join(key_paragraphs)
        
        # If still too long, truncate words
        words = summary.split()
        if len(words) > max_words:
            summary = ' '.join(words[:max_words]) + '...'
        
        logger.info(f"üìä Content summarized: {current_words} ‚Üí {len(summary.split())} words")
        return summary
    
    def _generate_heuristic(self, title: str, description: str, content: str, max_scenes: int) -> list:
        """Fallback heuristic method"""
        
    def _generate_heuristic(self, title: str, description: str, content: str, max_scenes: int) -> list:
        """Fallback heuristic method"""
        # Split content into logical chunks
        chunks = self._split_content(content, max_scenes - 3)  # Leave room for intro + summary + conclusion
        
        # Create pure narrative arc (no CTA, no product pitch)
        script = []
        
        # Scene 1: Hook/Introduction (just intro, no CTA)
        intro = f"{title}\n\n{description}"
        script.append(intro)
        
        # Scenes 2-N: Main content (pure storytelling)
        for i, chunk in enumerate(chunks, 1):
            scene_text = self._chunk_to_narration(chunk, i, len(chunks))
            if scene_text:
                script.append(scene_text)
        
        # Scene N-1: Summary/Key Takeaways
        if len(chunks) > 0:
            summary = self._generate_summary(chunks, title)
            if summary:
                script.append(summary)
        
        # Final scene: Conclusion with advice
        if len(script) > 1:
            conclusion = self._generate_conclusion(title, content)
            script.append(conclusion)
        
        logger.info(f"‚úÖ Generated {len(script)} heuristic scenes")
        return script[:max_scenes]
    
    def _generate_summary(self, chunks: list, title: str) -> str:
        """Generate summary of key points"""
        if not chunks:
            return ""
        
        # Extract key phrases from chunks
        key_points = []
        for chunk in chunks[:3]:  # First 3 chunks
            words = chunk.split()
            # Get first meaningful sentence
            if len(words) > 0:
                key_points.append(words[0])
        
        if key_points:
            return f"T√≥m l·∫°i, nh·ªØng ƒëi·ªÉm ch√≠nh c·ªßa '{title}' l√†: {', '.join(set(key_points[:3]))}. ƒê√≥ l√† nh·ªØng b√†i h·ªçc qu√Ω gi√° m√† ch√∫ng ta c√≥ th·ªÉ √°p d·ª•ng v√†o cu·ªôc s·ªëng h√†ng ng√†y."
        
        return "Nh·ªØng ƒëi·ªÉm ch√≠nh t·ª´ b√†i vi·∫øt n√†y s·∫Ω gi√∫p b·∫°n c√≥ c√°i nh√¨n s√¢u s·∫Øc h∆°n v·ªÅ v·∫•n ƒë·ªÅ."
    
    def _generate_conclusion(self, title: str, content: str) -> str:
        """Generate inspiring conclusion with advice"""
        # Analyze content sentiment/type
        content_lower = content.lower()
        
        # Different conclusions based on content
        if any(word in content_lower for word in ['h·ªçc', 'b√†i h·ªçc', 'kinh nghi·ªám']):
            return f"Hy v·ªçng qua '{title}', b·∫°n ƒë√£ h·ªçc ƒë∆∞·ª£c nh·ªØng ƒëi·ªÅu b·ªï √≠ch. H√£y √°p d·ª•ng nh·ªØng ki·∫øn th·ª©c n√†y v√†o cu·ªôc s·ªëng ƒë·ªÉ th·∫•y s·ª± thay ƒë·ªïi t√≠ch c·ª±c. C·∫£m ∆°n v√¨ ƒë√£ l·∫Øng nghe v√† ch√∫c b·∫°n th√†nh c√¥ng!"
        elif any(word in content_lower for word in ['c√¢u chuy·ªán', 'chuy·ªán', 's·ª± ki·ªán']):
            return f"C√¢u chuy·ªán n√†y cho ta th·∫•y r·∫±ng m·ªói tr·∫£i nghi·ªám ƒë·ªÅu c√≥ gi√° tr·ªã ri√™ng. H√£y suy ng·∫´m v√† t√¨m c√°ch ·ª©ng d·ª•ng v√†o t√¨nh hu·ªëng c·ªßa ch√≠nh m√¨nh. C·∫£m ∆°n c√°c b·∫°n ƒë√£ theo d√µi!"
        elif any(word in content_lower for word in ['l·ª£i √≠ch', 't√°c d·ª•ng', 'c√°ch']):
            return f"Nh·ªØng l·ª£i √≠ch v√† c√°ch ti·∫øp c·∫≠n t·ª´ '{title}' ch·∫Øc ch·∫Øn s·∫Ω gi√∫p √≠ch cho b·∫°n. H√£y th·ª≠ √°p d·ª•ng v√† chia s·∫ª k·∫øt qu·∫£ v·ªõi m·ªçi ng∆∞·ªùi. C·∫£m ∆°n ƒë√£ xem v√† ch√∫c b·∫°n may m·∫Øn!"
        else:
            return f"B√†i vi·∫øt '{title}' ƒë√£ mang ƒë·∫øn nhi·ªÅu th√¥ng tin b·ªï √≠ch. H√£y d√†nh th·ªùi gian suy ng·∫´m v√† t√¨m c√°ch √°p d·ª•ng v√†o cu·ªôc s·ªëng c·ªßa b·∫°n. C·∫£m ∆°n v√¨ ƒë√£ theo d√µi ch√∫ng t√¥i!"
    
    def _split_content(self, content: str, max_chunks: int) -> list:
        """Split content into logical paragraphs/chunks"""
        # Split by double newline (paragraphs)
        paragraphs = content.split('\n\n')
        
        # Remove empty paragraphs
        paragraphs = [p.strip() for p in paragraphs if p.strip()]
        
        # If too many, group them
        if len(paragraphs) > max_chunks:
            return self._group_paragraphs(paragraphs, max_chunks)
        
        return paragraphs
    
    def _group_paragraphs(self, paragraphs: list, target_groups: int) -> list:
        """Group paragraphs into target number of chunks"""
        if not paragraphs or target_groups < 1:
            return paragraphs
        
        # Calculate items per group
        items_per_group = len(paragraphs) // target_groups
        if items_per_group < 1:
            items_per_group = 1
        
        groups = []
        current_group = []
        
        for para in paragraphs:
            current_group.append(para)
            if len(current_group) >= items_per_group:
                groups.append('\n\n'.join(current_group))
                current_group = []
        
        # Add remaining
        if current_group:
            if groups:
                groups[-1] += '\n\n' + '\n\n'.join(current_group)
            else:
                groups.append('\n\n'.join(current_group))
        
        return groups[:target_groups]
    
    def _chunk_to_narration(self, chunk: str, chunk_num: int, total_chunks: int) -> str:
        """Convert a content chunk into a natural narration"""
        # Remove extra spaces
        chunk = ' '.join(chunk.split())
        
        # Limit to ~100-150 words per scene (~20-30 seconds when spoken)
        words = chunk.split()
        if len(words) > 150:
            words = words[:150]
        
        narration = ' '.join(words)
        
        # Add transition if not first/last
        if chunk_num > 1 and chunk_num < total_chunks:
            narration = f"Ti·∫øp theo, {narration}"
        elif chunk_num > 1:
            narration = f"Cu·ªëi c√πng, {narration}"
        
        return narration
